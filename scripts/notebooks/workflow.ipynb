{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def head_tokenizer(string_value, header_charater):\n",
    "    \"\"\"Tokenize a row composed of tokens separated by spaces, disregarding tokens starting with a specific charter\n",
    "\n",
    "    Args:\n",
    "        string_value (str): The string to tokenize.\n",
    "        header_charater (str): The charter defining an header row.\n",
    "\n",
    "    Returns:\n",
    "        item_positions (dict): A dictionary with tokens as keys and positions of the tokens as values.\n",
    "    \"\"\"\n",
    "\n",
    "    positions = []\n",
    "    items = []\n",
    "    for m in re.finditer(r\"\\S+\", string_value):\n",
    "        position, item = m.start(), m.group()\n",
    "        if item != header_charater:\n",
    "            items.append(item)\n",
    "            positions.append(position)\n",
    "\n",
    "    item_positions = dict()\n",
    "    for i in range(len(items) - 1):\n",
    "        if i == 0:\n",
    "            item_positions[items[i]] = [0, positions[i + 1] - 1]\n",
    "            continue\n",
    "        item_positions[items[i]] = [positions[i], positions[i + 1] - 1]\n",
    "\n",
    "    item_positions[items[-1]] = positions[-1], len(string_value)\n",
    "\n",
    "    return item_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class ValueTypes(Enum):\n",
    "    TimeStamp = 0\n",
    "    String = 1\n",
    "    Integer = 2\n",
    "    Float = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Row tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def cast_string_value_to_type(string_value, string_type):\n",
    "    \"\"\"Casts a string to a value, as defined by the value type.\n",
    "\n",
    "    Args:\n",
    "        string_value (str): The string to cast.\n",
    "        string_type (ValueTypes): The type of the casting operation.\n",
    "    \"\"\"\n",
    "\n",
    "    result = None\n",
    "\n",
    "    if string_value == \"\":\n",
    "        return result\n",
    "\n",
    "    if string_type == ValueTypes.TimeStamp:\n",
    "        result = dt.strptime(string_value, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    if string_type == ValueTypes.Float:\n",
    "        result = float(string_value)\n",
    "\n",
    "    if string_type == ValueTypes.String:\n",
    "        result = str(string_value)\n",
    "\n",
    "    if string_type == ValueTypes.Integer:\n",
    "        result = int(string_value)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def row_tokenizer(string_value, item_positions, column_names, column_types):\n",
    "    \"\"\"Finds the tokens on a row at specified positions.\n",
    "\n",
    "    Args:\n",
    "        string_value (str): The string to tokenize.\n",
    "        item_positions (dict): A dictionary with the tokens as keys and the positions of the tokens as values.\n",
    "        column_names (list): The column names of each token\n",
    "        column_types (list): The column types of each token\n",
    "    \"\"\"\n",
    "\n",
    "    result = []\n",
    "    num_requested_items = len(column_names)\n",
    "\n",
    "    for column_name in column_names:\n",
    "\n",
    "        if len(result) == num_requested_items:\n",
    "            break\n",
    "\n",
    "        if column_name not in item_positions.keys():\n",
    "            continue\n",
    "\n",
    "        var_value = string_value[\n",
    "            item_positions[column_name][0] : item_positions[column_name][1]\n",
    "        ].strip()\n",
    "\n",
    "        result.append(cast_string_value_to_type(var_value, column_types[column_name]))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileParser:\n",
    "    \"\"\"Class parsing a large text file with PySpark\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path,\n",
    "        spark_context,\n",
    "        header_charater,\n",
    "        filter_column,\n",
    "        filter_value,\n",
    "        header_estimated_length,\n",
    "        column_names,\n",
    "        column_types,\n",
    "    ):\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The file path.\n",
    "            spark_context (sparkContext): The spark context.\n",
    "            header_charater (str): The symbol defining an header row\n",
    "            filter_column (str): The column to use for filtering the file rows\n",
    "            filter_value (str): The value to use for filtering the file rows\n",
    "            header_estimated_length (int): An estimated initial header length.\n",
    "            column_names (list): The names of the columns to extract.\n",
    "            column_types (list): The column types.\n",
    "        \"\"\"\n",
    "\n",
    "        self.rdd = spark_context.textFile(file_path)\n",
    "        self.file_path = file_path\n",
    "\n",
    "        self.header_charater = header_charater\n",
    "        self.header_estimated_length = header_estimated_length\n",
    "        self.column_names = column_names\n",
    "        self.column_types = column_types\n",
    "        self.filter_column = filter_column\n",
    "        self.filter_value = filter_value\n",
    "\n",
    "        self.first_rows = self.rdd.take(self.header_estimated_length)\n",
    "\n",
    "    def parse(self):\n",
    "        \"\"\"Implements the parsing operations.\n",
    "        First the number of header rows is determined by loading only the estimated number of row headers and the last\n",
    "        row containing the column names is tokenized.\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # assume contained in the first self.length_header rows\n",
    "        if self.first_rows[-1] == self.header_charater:\n",
    "            raise ValueError(\n",
    "                \"Estimated length of the header is too small, please increase it\"\n",
    "            )\n",
    "\n",
    "        num_header_rows = 0\n",
    "        for row in self.first_rows:\n",
    "            if row[0] == self.header_charater:\n",
    "                num_header_rows += 1\n",
    "\n",
    "        header = self.first_rows[num_header_rows - 1]\n",
    "        item_positions = head_tokenizer(header, self.header_charater)\n",
    "\n",
    "        all_items_found = all(\n",
    "            item in item_positions.keys() for item in self.column_names\n",
    "        )\n",
    "        if not all_items_found:\n",
    "            raise ValueError(\"Not all required columns are found for \" + self.file_path)\n",
    "\n",
    "        header_symbol = self.header_charater\n",
    "        column_names = self.column_names\n",
    "        column_types = self.column_types\n",
    "        header_symbol = self.header_charater\n",
    "        filter_column = self.filter_column\n",
    "        filter_value = self.filter_value\n",
    "\n",
    "        return self.rdd.filter(\n",
    "            lambda line: line[0] != header_symbol\n",
    "            and line[\n",
    "                item_positions[filter_column][0] : item_positions[filter_column][1]\n",
    "            ].strip()\n",
    "            == filter_value\n",
    "        ).map(lambda x: row_tokenizer(x, item_positions, column_names, column_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_union(dataframes_list):\n",
    "    while len(dataframes_list) > 1:\n",
    "        unified_df = [\n",
    "            df1.union(df2).distinct()\n",
    "            for df1, df2 in zip(dataframes_list[::2], dataframes_list[1::2])\n",
    "        ]\n",
    "        if len(dataframes_list) > 1 and len(unified_df) % 2 == 1:\n",
    "            unified_df[-1] = unified_df[-1].union(dataframes_list[-1]).distinct()\n",
    "        dataframes_list = unified_df\n",
    "    return unified_df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pyspark_submit_args = \"--executor-memory 8g pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "os.environ['JAVA_HOME'] = 'D:\\Apps\\java_jre_1_8_0_301'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import (\n",
    "    FloatType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")\n",
    "spark = SparkSession.builder.appName(\"compute_heat_waves\").getOrCreate()\n",
    "spark_context = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"DTG\", \"NAME\", \"TX_DRYB_10\"]\n",
    "column_types = {\n",
    "    column_names[0]: ValueTypes.TimeStamp,\n",
    "    column_names[1]: ValueTypes.String,\n",
    "    column_names[2]: ValueTypes.Float,\n",
    "}\n",
    "data_frame_schema = [\n",
    "    StructField(column_names[0], TimestampType(), True),\n",
    "    StructField(column_names[1], StringType(), True),\n",
    "    StructField(column_names[2], FloatType(), True),\n",
    "]\n",
    "schema = StructType(data_frame_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile, join\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "dir_path = \"../../data/raw_data\"\n",
    "abs_dir_path = os.path.abspath(dir_path)\n",
    "all_files_path = [\n",
    "    join(abs_dir_path, f)\n",
    "    for f in listdir(abs_dir_path)\n",
    "    if isfile(join(abs_dir_path, f))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, to_date, countDistinct, col, row_number\n",
    "from pyspark.sql.functions import max as pyspark_max\n",
    "from pyspark.sql.functions import min as pyspark_min\n",
    "from pyspark.sql.window import Window\n",
    "dfs = []\n",
    "for iteration, file_path in enumerate(all_files_path):\n",
    "    try:\n",
    "        file_parser = FileParser(\n",
    "            file_path=file_path,\n",
    "            spark_context=spark_context,\n",
    "            header_charater=\"#\",\n",
    "            filter_column=\"NAME\",\n",
    "            filter_value=\"De Bilt\",\n",
    "            header_estimated_length=100,\n",
    "            column_names=column_names,\n",
    "            column_types=column_types,\n",
    "        )\n",
    "        spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \"\\t\").schema(schema).load(file_path)\n",
    "        \n",
    "        \n",
    "        df = file_parser.parse().toDF(schema=schema)\n",
    "        print(\"Iteration {}\".format(iteration))\n",
    "        dfs.append(df)\n",
    "    except ValueError:\n",
    "        print(\"error found for file {}, iteration {}\".format(file_path, iteration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the union of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#union_df= None\n",
    "#for i,df in enumerate(dfs):\n",
    "    \n",
    "#    if i==0:\n",
    "#        union_df = df\n",
    "#        continue\n",
    "        \n",
    "#    union_df = union_df.union(df).distinct()\n",
    "    \n",
    "union_df = pairwise_union(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a dates column, count the dinstict timestamps within the day, find the maximum temp, order by dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_df = (union_df.withColumn(\"Dates\", to_date(col(\"DTG\"))).groupBy(\"Dates\").agg(countDistinct(\"DTG\"), pyspark_max(\"TX_DRYB_10\"), pyspark_min(\"TX_DRYB_10\")).orderBy(\"Dates\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#union_df_final.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_df_final_pd = union_df.toPandas().set_index(\"Dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_df_final_pd.to_csv(\"reduced_panda_df.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now print the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(DTG)</th>\n",
       "      <th>max(TX_DRYB_10)</th>\n",
       "      <th>min(TX_DRYB_10)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dates</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2003-04-01</th>\n",
       "      <td>143</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-04-02</th>\n",
       "      <td>144</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-04-03</th>\n",
       "      <td>144</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-04-04</th>\n",
       "      <td>144</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>-2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-04-05</th>\n",
       "      <td>144</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-28</th>\n",
       "      <td>144</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-29</th>\n",
       "      <td>144</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30</th>\n",
       "      <td>144</td>\n",
       "      <td>18.299999</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>138</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-01</th>\n",
       "      <td>1</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5839 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            count(DTG)  max(TX_DRYB_10)  min(TX_DRYB_10)\n",
       "Dates                                                   \n",
       "2003-04-01         143        13.200000             -0.2\n",
       "2003-04-02         144        10.000000              3.6\n",
       "2003-04-03         144         9.300000              0.5\n",
       "2003-04-04         144        10.800000             -2.2\n",
       "2003-04-05         144        11.600000              5.3\n",
       "...                ...              ...              ...\n",
       "2019-03-28         144        11.200000              3.2\n",
       "2019-03-29         144        17.100000              0.4\n",
       "2019-03-30         144        18.299999              4.0\n",
       "2019-03-31         138        12.500000              3.7\n",
       "2019-04-01           1         3.500000              3.5\n",
       "\n",
       "[5839 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_df_final_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 25\n",
    "duration = 5\n",
    "duration_max_temperature = 3\n",
    "max_temperature = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "duration = 5\n",
    "index_deque = deque(maxlen=duration)\n",
    "are_last_days_hot = False\n",
    "start_hot_days = None\n",
    "end_hot_days = None\n",
    "num_tropical_days = 0\n",
    "tropical_days = []\n",
    "max_temp_hot_days = -1000\n",
    "heat_waves_durations = []\n",
    "tropical_days_in_heat_wave = []\n",
    "for date in union_df_final_pd.index:\n",
    "    index_deque.append(date)\n",
    "    if len(index_deque) == duration:\n",
    "        df_slice = df.loc[index_deque]\n",
    "        min_temp = np.min(df_slice['max(TX_DRYB_10)'].values)\n",
    "        max_temp = np.max(df_slice['max(TX_DRYB_10)'].values)\n",
    "\n",
    "        if min_temp>temperature  and not are_last_days_hot:\n",
    "            num_tropical_days = num_tropical_days + len(df_slice.loc[df['max(TX_DRYB_10)']>max_temperature].values)\n",
    "            start_hot_days = index_deque[0]\n",
    "            end_hot_days = index_deque[-1]\n",
    "        if min_temp>temperature and are_last_days_hot:\n",
    "            end_hot_days = index_deque[-1]\n",
    "            if (df_slice.loc[index_deque[-1]]['max(TX_DRYB_10)']>max_temperature):\n",
    "                num_tropical_days = num_tropical_days + 1\n",
    "        if min_temp<=temperature and are_last_days_hot:\n",
    "            are_last_days_hot = False\n",
    "            if num_tropical_days >duration_max_temperature:\n",
    "                tropical_days_in_heat_wave.append(num_tropical_days)\n",
    "                heat_waves_durations.append([start_hot_days, end_hot_days])\n",
    "                \n",
    "print(heat_wave_start)\n",
    "print(heat_wave_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
