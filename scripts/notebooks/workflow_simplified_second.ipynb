{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def head_split(header):\n",
    "    \"\"\"Tokenize the header in items and find the column boundaries. An header always starts with #\n",
    "\n",
    "    Args:\n",
    "        header (str): The header to tokenize.\n",
    "        item_positions (dict): A dictionary with the tokens as keys and a list with the start\n",
    "         and end positions of the token as values.\n",
    "    \"\"\"\n",
    "\n",
    "    positions = []\n",
    "    items = []\n",
    "    for m in re.finditer(r\"\\S+\", header):\n",
    "        position, item = m.start(), m.group()\n",
    "        if item != \"#\":\n",
    "            items.append(item)\n",
    "            positions.append(position)\n",
    "\n",
    "    item_positions = dict()\n",
    "    for i in range(len(items) - 1):\n",
    "        if i == 0:\n",
    "            item_positions[items[i]] = [0, positions[i + 1] - 1]\n",
    "            continue\n",
    "        item_positions[items[i]] = [positions[i], positions[i + 1] - 1]\n",
    "\n",
    "    item_positions[items[-1]] = positions[-1], len(header)\n",
    "\n",
    "    return item_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class ValueTypes(Enum):\n",
    "    TimeStamp = 0\n",
    "    String = 1\n",
    "    Integer = 2\n",
    "    Float = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Row splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from numba import jit\n",
    "\n",
    "def cast_string_value_to_type(string_value, string_type):\n",
    "    \"\"\"Casts a string to a value, as defined by the value type.\n",
    "\n",
    "    Args:\n",
    "        string_value (str): The string to cast.\n",
    "        string_type (ValueTypes): The type of the casting operation.\n",
    "    \"\"\"\n",
    "\n",
    "    result = None\n",
    "\n",
    "    if string_value == \"\":\n",
    "        return result\n",
    "\n",
    "    if string_type == ValueTypes.TimeStamp:\n",
    "        result = dt.strptime(string_value, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    if string_type == ValueTypes.Float:\n",
    "        result = float(string_value)\n",
    "\n",
    "    if string_type == ValueTypes.String:\n",
    "        result = str(string_value)\n",
    "\n",
    "    if string_type == ValueTypes.Integer:\n",
    "        result = int(string_value)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def row_split(row, item_positions, column_names, column_types):\n",
    "    \"\"\"Finds the names on a row at specified positions.\n",
    "\n",
    "    Args:\n",
    "        row (str): The header to tokenize.\n",
    "        item_positions (dict): A dictionary with the tokens as keys and a list with the start\n",
    "         and end positions of the token as values.\n",
    "        column_names (): The names of the columns\n",
    "        column_types ():\n",
    "    \"\"\"\n",
    "\n",
    "    result = []\n",
    "    num_requested_items = len(column_names)\n",
    "\n",
    "    for column_name in column_names:\n",
    "\n",
    "        if len(result) == num_requested_items:\n",
    "            break\n",
    "\n",
    "        if column_name not in item_positions.keys():\n",
    "            continue\n",
    "\n",
    "        var_value = row[\n",
    "            item_positions[column_name][0] : item_positions[column_name][1]\n",
    "        ].strip()\n",
    "\n",
    "        result.append(cast_string_value_to_type(var_value, column_types[column_name]))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileParser:\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path,\n",
    "        spark_context,\n",
    "        header_symbol,\n",
    "        filter_column,\n",
    "        filter_value,\n",
    "        header_estimated_length,\n",
    "        column_names,\n",
    "        column_types,\n",
    "    ):\n",
    "        self.rdd = spark_context.textFile(file_path, minPartitions=500)\n",
    "        self.file_path = file_path\n",
    "\n",
    "        self.header_symbol = header_symbol\n",
    "        self.header_estimated_length = header_estimated_length\n",
    "        self.column_names = column_names\n",
    "        self.column_types = column_types\n",
    "        self.filter_column = filter_column\n",
    "        self.filter_value = filter_value\n",
    "\n",
    "        self.first_rows = self.rdd.take(self.header_estimated_length)\n",
    "\n",
    "    def get_temporal_extend(self):\n",
    "        self.header_extract_period_function(self.first_rows)\n",
    "\n",
    "    def parse(self):\n",
    "\n",
    "        # assume contained in the first self.length_header rows\n",
    "        if self.first_rows[-1] == self.header_symbol:\n",
    "            raise ValueError(\n",
    "                \"Estimated length of the header is too small, please increase it\"\n",
    "            )\n",
    "\n",
    "        num_header_rows = 0\n",
    "        for row in self.first_rows:\n",
    "            if row[0] == self.header_symbol:\n",
    "                num_header_rows += 1\n",
    "\n",
    "        header = self.first_rows[num_header_rows - 1]\n",
    "        item_positions = head_split(header)\n",
    "        all_items_found = all(\n",
    "            item in item_positions.keys() for item in self.column_names\n",
    "        )\n",
    "\n",
    "        if not all_items_found:\n",
    "            raise ValueError(\"Not all required columns are found for \" + self.file_path)\n",
    "\n",
    "        header_symbol = self.header_symbol\n",
    "        column_names = self.column_names\n",
    "        column_types = self.column_types\n",
    "        header_symbol = self.header_symbol\n",
    "        filter_column = self.filter_column\n",
    "        filter_value = self.filter_value\n",
    "\n",
    "        return self.rdd.filter(\n",
    "            lambda line: line[0] != header_symbol and line[item_positions[filter_column][0] : item_positions[filter_column][1]].strip()== filter_value\n",
    "        ).map(lambda x: row_split(x, item_positions, column_names, column_types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_union(dataframes_list):\n",
    "    while len(dataframes_list) > 1:\n",
    "        unified_df = [\n",
    "            df1.union(df2).distinct()\n",
    "            for df1, df2 in zip(dataframes_list[::2], dataframes_list[1::2])\n",
    "        ]\n",
    "        if len(dataframes_list) > 1 and len(unified_df) % 2 == 1:\n",
    "            unified_df[-1] = unified_df[-1].union(dataframes_list[-1]).distinct()\n",
    "        dataframes_list = unified_df\n",
    "    return unified_df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import (\n",
    "    FloatType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('gdd').getOrCreate()\n",
    "#spark.conf.set(\"spark.executor.memory\", \"10g\")\n",
    "#spark.conf.set(\"spark.executor.cores\", \"4\")\n",
    "#spark_context = spark.sparkContext\n",
    "#n_workers =  len([executor.host() for executor in spark_context.statusTracker().getExecutorInfos() ]) -1\n",
    "#print(spark.SparkConf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"compute_heat_waves\").getOrCreate()\n",
    "spark_context = spark.sparkContext\n",
    "\n",
    "column_names = [\"DTG\", \"NAME\", \"TX_DRYB_10\"]\n",
    "column_types = {\n",
    "    column_names[0]: ValueTypes.TimeStamp,\n",
    "    column_names[1]: ValueTypes.String,\n",
    "    column_names[2]: ValueTypes.Float,\n",
    "}\n",
    "data_frame_schema = [\n",
    "    StructField(column_names[0], TimestampType(), True),\n",
    "    StructField(column_names[1], StringType(), True),\n",
    "    StructField(column_names[2], FloatType(), True),\n",
    "]\n",
    "schema = StructType(data_frame_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile, join\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "dir_path = \"../../data/uncompressed\"\n",
    "abs_dir_path = os.path.abspath(dir_path)\n",
    "all_files_path = [\n",
    "    join(abs_dir_path, f)\n",
    "    for f in listdir(abs_dir_path)\n",
    "    if isfile(join(abs_dir_path, f))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, to_date, countDistinct, col, row_number\n",
    "from pyspark.sql.functions import max as pyspark_max\n",
    "from pyspark.sql.functions import min as pyspark_min\n",
    "from pyspark.sql.window import Window\n",
    "dfs = []\n",
    "for iteration, file_path in enumerate(all_files_path[:2]):\n",
    "    try:\n",
    "        file_parser = FileParser(\n",
    "            file_path=file_path,\n",
    "            spark_context=spark_context,\n",
    "            header_symbol=\"#\",\n",
    "            filter_column='NAME',\n",
    "            filter_value='De Bilt',\n",
    "            header_estimated_length=100,\n",
    "            column_names=column_names,\n",
    "            column_types=column_types,\n",
    "        )\n",
    "        df = file_parser.parse().toDF(schema=schema)\n",
    "        print(\"Iteration {}\".format(iteration))\n",
    "        dfs.append(df)\n",
    "    except ValueError:\n",
    "        print(\"error found for file {}, iteration {}\".format(file_path, iteration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 5\n",
    "duration = 5\n",
    "max_temperature = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the union of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_df = pairwise_union(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a dates column, count the dinstict timestamps within the day, find the maximum temp, order by dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_df = (union_df.withColumn(\"Dates\", to_date(col(\"DTG\"))).groupBy(\"Dates\").agg(countDistinct(\"DTG\"), pyspark_max(\"TX_DRYB_10\"), pyspark_min(\"TX_DRYB_10\")).orderBy(\"Dates\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#union_df_final.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_df_final_pd = union_df.toPandas().set_index(\"Dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_df_final_pd.to_csv(\"reduced_panda_df.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now print the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(DTG)</th>\n",
       "      <th>max(TX_DRYB_10)</th>\n",
       "      <th>min(TX_DRYB_10)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dates</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2003-04-01</th>\n",
       "      <td>143</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-04-02</th>\n",
       "      <td>144</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-04-03</th>\n",
       "      <td>144</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-04-04</th>\n",
       "      <td>144</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>-2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-04-05</th>\n",
       "      <td>144</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-28</th>\n",
       "      <td>144</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-29</th>\n",
       "      <td>144</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-30</th>\n",
       "      <td>144</td>\n",
       "      <td>18.299999</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-31</th>\n",
       "      <td>138</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-01</th>\n",
       "      <td>1</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5839 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            count(DTG)  max(TX_DRYB_10)  min(TX_DRYB_10)\n",
       "Dates                                                   \n",
       "2003-04-01         143        13.200000             -0.2\n",
       "2003-04-02         144        10.000000              3.6\n",
       "2003-04-03         144         9.300000              0.5\n",
       "2003-04-04         144        10.800000             -2.2\n",
       "2003-04-05         144        11.600000              5.3\n",
       "...                ...              ...              ...\n",
       "2019-03-28         144        11.200000              3.2\n",
       "2019-03-29         144        17.100000              0.4\n",
       "2019-03-30         144        18.299999              4.0\n",
       "2019-03-31         138        12.500000              3.7\n",
       "2019-04-01           1         3.500000              3.5\n",
       "\n",
       "[5839 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_df_final_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[datetime.date(2003, 6, 1)]\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "is_high_temp = False\n",
    "high_temp_days_start = None\n",
    "high_temp_days_end = None\n",
    "running_max_temp = -1000.0\n",
    "for index, row in union_df_final_pd.iterrows():\n",
    "    if row['last_days_min_temp'] > temperature and not is_high_temp:\n",
    "        running_max_temp = max(running_max_temp, row['last_days_max_temp']) \n",
    "        high_temp_days_start = index - timedelta(duration)\n",
    "        high_temp_days_end = index\n",
    "        is_high_temp = True\n",
    "    if row['last_days_min_temp'] < temperature and is_high_temp:\n",
    "        high_temp_days_end = index\n",
    "        if running_max_temp > 35:\n",
    "            heat_wave_end.append([high_temp_days_start,high_temp_days_end ])\n",
    "        running_max_temp= -1000.0\n",
    "        high_temp_days_start = None\n",
    "        high_temp_days_end = None\n",
    "        is_high_temp = False\n",
    "\n",
    "#If we still are on a heat wave\n",
    "if is_in_heat_wave:\n",
    "    heat_wave_end.append(union_df_final_pd.index[-1])\n",
    "        \n",
    "print(heat_wave_start)\n",
    "print(heat_wave_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
