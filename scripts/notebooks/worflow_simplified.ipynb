{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def head_split(header):\n",
    "    positions = []\n",
    "    items = []\n",
    "    for m in re.finditer(r\"\\S+\", header):\n",
    "        position, item = m.start(), m.group()\n",
    "        if item != \"#\":\n",
    "            positions.append(position)\n",
    "            items.append(item)\n",
    "    # correct for initial #\n",
    "    positions[0] = 0\n",
    "\n",
    "    return positions, items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class ColumnTypes(Enum):\n",
    "    TimeStampType = 0\n",
    "    String = 1\n",
    "    Integer = 2\n",
    "    Float = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Row splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from numba import jit\n",
    "\n",
    "def cast_string_value_to_type(var_value, column_type):\n",
    "    result = None\n",
    "\n",
    "    if var_value == \"\":\n",
    "        return result\n",
    "\n",
    "    if column_type == ColumnTypes.TimeStampType:\n",
    "        result = dt.strptime(var_value, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    if column_type == ColumnTypes.Float:\n",
    "        result = float(var_value)\n",
    "\n",
    "    if column_type == ColumnTypes.String:\n",
    "        result = str(var_value)\n",
    "\n",
    "    if column_type == ColumnTypes.Integer:\n",
    "        result = int(var_value)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def row_split(r, positions, items, column_names, column_types):\n",
    "    result = []\n",
    "    num_items = len(positions)\n",
    "    num_requested_items = len(column_names)\n",
    "\n",
    "    for i in range(0, num_items - 1):\n",
    "\n",
    "        if len(result) == num_requested_items:\n",
    "            break\n",
    "\n",
    "        if items[i] not in column_names:\n",
    "            continue\n",
    "\n",
    "        var_value = r[positions[i] : positions[i + 1]].strip()\n",
    "\n",
    "        result.append(cast_string_value_to_type(var_value, column_types[items[i]]))\n",
    "\n",
    "    if items[-1] in column_names:\n",
    "        var_value = r[positions[num_items - 1] :].strip()\n",
    "\n",
    "        result.append(cast_string_value_to_type(var_value, column_types[items[-1]]))\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileParser:\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path,\n",
    "        spark_context,\n",
    "        header_symbol,\n",
    "        header_estimated_length,\n",
    "        column_names,\n",
    "        column_types,\n",
    "    ):\n",
    "        self.rdd = spark_context.textFile(file_path)\n",
    "        self.file_path = file_path\n",
    "\n",
    "        self.header_symbol = header_symbol\n",
    "        self.header_estimated_length = header_estimated_length\n",
    "        self.column_names = column_names\n",
    "        self.column_types = column_types\n",
    "\n",
    "        self.first_rows = self.rdd.take(self.header_estimated_length)\n",
    "\n",
    "    def get_temporal_extend(self):\n",
    "        self.header_extract_period_function(self.first_rows)\n",
    "\n",
    "    def parse(self):\n",
    "\n",
    "        # assume contained in the first self.length_header rows\n",
    "        if self.first_rows[-1] == self.header_symbol:\n",
    "            raise ValueError(\n",
    "                \"Estimated length of the header is too small, please increase it\"\n",
    "            )\n",
    "\n",
    "        num_header_rows = 0\n",
    "        for row in self.first_rows:\n",
    "            if row[0] == self.header_symbol:\n",
    "                num_header_rows += 1\n",
    "\n",
    "        header = self.first_rows[num_header_rows - 1]\n",
    "        positions, items = head_split(header)\n",
    "        all_items_found = all(item in items for item in self.column_names)\n",
    "\n",
    "        if not all_items_found:\n",
    "            raise ValueError(\"Not all required columns ar found for \" + self.file_path)\n",
    "\n",
    "        column_names = self.column_names\n",
    "        column_types = self.column_types\n",
    "        header_symbol = self.header_symbol\n",
    "        return self.rdd.filter(lambda line: line[0] != header_symbol && line[0]).map(\n",
    "            lambda x: row_split(x, positions, items, column_names, column_types)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_union(dataframes_list):\n",
    "    while len(dataframes_list) > 1:\n",
    "        unified_df = [\n",
    "            df1.union(df2).distinct()\n",
    "            for df1, df2 in zip(dataframes_list[::2], dataframes_list[1::2])\n",
    "        ]\n",
    "        if len(dataframes_list) > 1 and len(unified_df) % 2 == 1:\n",
    "            unified_df[-1] = unified_df[-1].union(dataframes_list[-1]).distinct()\n",
    "        dataframes_list = unified_df\n",
    "    return unified_df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import (\n",
    "    FloatType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    "    TimestampType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('gdd').getOrCreate()\n",
    "#spark.conf.set(\"spark.executor.memory\", \"10g\")\n",
    "#spark.conf.set(\"spark.executor.cores\", \"4\")\n",
    "#spark_context = spark.sparkContext\n",
    "#n_workers =  len([executor.host() for executor in spark_context.statusTracker().getExecutorInfos() ]) -1\n",
    "#print(spark.SparkConf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"compute_heat_waves\").getOrCreate()\n",
    "spark_context = spark.sparkContext\n",
    "\n",
    "column_names = [\"DTG\", \"NAME\", \"TX_DRYB_10\"]\n",
    "column_types = {\n",
    "    column_names[0]: ColumnTypes.TimeStampType,\n",
    "    column_names[1]: ColumnTypes.String,\n",
    "    column_names[2]: ColumnTypes.Float,\n",
    "}\n",
    "data_frame_schema = [\n",
    "    StructField(column_names[0], TimestampType(), True),\n",
    "    StructField(column_names[1], StringType(), True),\n",
    "    StructField(column_names[2], FloatType(), True),\n",
    "]\n",
    "schema = StructType(data_frame_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isfile, join\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "dir_path = \"../../data/uncompressed\"\n",
    "abs_dir_path = os.path.abspath(dir_path)\n",
    "all_files_path = [\n",
    "    join(abs_dir_path, f)\n",
    "    for f in listdir(abs_dir_path)\n",
    "    if isfile(join(abs_dir_path, f))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, to_date, countDistinct, col, row_number\n",
    "from pyspark.sql.functions import max as pyspark_max\n",
    "from pyspark.sql.functions import min as pyspark_min\n",
    "from pyspark.sql.window import Window\n",
    "dfs = []\n",
    "for iteration, file_path in enumerate(all_files_path):\n",
    "    try:\n",
    "        file_parser = FileParser(\n",
    "            file_path=file_path,\n",
    "            spark_context=spark_context,\n",
    "            header_symbol=\"#\",\n",
    "            header_estimated_length=100,\n",
    "            column_names=column_names,\n",
    "            column_types=column_types,\n",
    "        )\n",
    "        df = file_parser.parse().toDF(schema=schema).filter(\"NAME=='De Bilt'\")\n",
    "        print(\"Iteration {}\".format(iteration))\n",
    "        dfs.append(df)\n",
    "    except ValueError:\n",
    "        print(\"error found for file {}, iteration {}\".format(file_path, iteration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 15\n",
    "duration = 5\n",
    "max_temperature = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the union of dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_df = pairwise_union(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a dates column, count the dinstict timestamps within the day, find the maximum temp, order by dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_df = (union_df.withColumn(\"Dates\", to_date(col(\"DTG\"))).groupBy(\"Dates\").agg(countDistinct(\"DTG\"), pyspark_max(\"TX_DRYB_10\"), pyspark_min(\"TX_DRYB_10\")).orderBy(\"Dates\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#union_df_final.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_df_final_pd = union_df.toPandas().set_index(\"Dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_df_final_pd.to_csv(\"reduced_panda_df.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now print the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(DTG)</th>\n",
       "      <th>max(TX_DRYB_10)</th>\n",
       "      <th>min(TX_DRYB_10)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dates</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2003-04-01</th>\n",
       "      <td>143</td>\n",
       "      <td>13.200000</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-04-02</th>\n",
       "      <td>144</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-04-03</th>\n",
       "      <td>144</td>\n",
       "      <td>9.300000</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-04-04</th>\n",
       "      <td>144</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>-2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-04-05</th>\n",
       "      <td>144</td>\n",
       "      <td>11.600000</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-05-28</th>\n",
       "      <td>144</td>\n",
       "      <td>23.700001</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-05-29</th>\n",
       "      <td>144</td>\n",
       "      <td>25.900000</td>\n",
       "      <td>10.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-05-30</th>\n",
       "      <td>144</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>12.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-05-31</th>\n",
       "      <td>144</td>\n",
       "      <td>25.100000</td>\n",
       "      <td>13.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-06-01</th>\n",
       "      <td>1</td>\n",
       "      <td>15.800000</td>\n",
       "      <td>15.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            count(DTG)  max(TX_DRYB_10)  min(TX_DRYB_10)\n",
       "Dates                                                   \n",
       "2003-04-01         143        13.200000             -0.2\n",
       "2003-04-02         144        10.000000              3.6\n",
       "2003-04-03         144         9.300000              0.5\n",
       "2003-04-04         144        10.800000             -2.2\n",
       "2003-04-05         144        11.600000              5.3\n",
       "...                ...              ...              ...\n",
       "2003-05-28         144        23.700001              9.3\n",
       "2003-05-29         144        25.900000             10.6\n",
       "2003-05-30         144        27.500000             12.8\n",
       "2003-05-31         144        25.100000             13.5\n",
       "2003-06-01           1        15.800000             15.8\n",
       "\n",
       "[62 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_df_final_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[datetime.date(2003, 6, 1)]\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "is_high_temp = False\n",
    "high_temp_days_start = None\n",
    "high_temp_days_end = None\n",
    "running_max_temp = -1000.0\n",
    "for index, row in union_df_final_pd.iterrows():\n",
    "    if row['last_days_min_temp'] > temperature and not is_high_temp:\n",
    "        running_max_temp = max(running_max_temp, row['last_days_max_temp']) \n",
    "        high_temp_days_start = index - timedelta(duration)\n",
    "        high_temp_days_end = index\n",
    "        is_high_temp = True\n",
    "    if row['last_days_min_temp'] < temperature and is_high_temp:\n",
    "        high_temp_days_end = index\n",
    "        if running_max_temp > 35:\n",
    "            heat_wave_end.append([high_temp_days_start,high_temp_days_end ])\n",
    "        running_max_temp= -1000.0\n",
    "        high_temp_days_start = None\n",
    "        high_temp_days_end = None\n",
    "        is_high_temp = False\n",
    "\n",
    "#If we still are on a heat wave\n",
    "if is_in_heat_wave:\n",
    "    heat_wave_end.append(union_df_final_pd.index[-1])\n",
    "        \n",
    "print(heat_wave_start)\n",
    "print(heat_wave_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
